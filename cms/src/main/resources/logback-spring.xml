<?xml version="1.0" encoding="UTF-8"?>
<!-- 日志级别从低到高分为TRACE < DEBUG < INFO < WARN < ERROR < FATAL，如果设置为WARN，则低于WARN的信息都不会输出 -->
<!-- scan:当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true -->
<!-- scanPeriod:设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟。 -->
<!-- debug:当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。 -->
<configuration  scan="true" scanPeriod="10 seconds">
    <springProperty scope="context" name="logPath" source="logPath" defaultValue="${user.dir}/logs"/>

    <!--输出到控制台-->
    <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>error</level>
        </filter>
        <encoder>
            <pattern>%yellow(%d{yyyy-MM-dd HH:mm:ss}) %red([%thread]) %highlight(%-5level) %cyan(%logger{50}) - %magenta(%msg) %n
            </pattern>            <!--输出格式-->
            <!--%d{HH:mm:ss.SSS} 输出时间-->
            <!--[%thread] 输出的线程名称-->
            <!--%-5level 输出的日志级别 靠左占5个字符-->
            <!--%logger{36} 输出日志输出者-->
            <!--%msg 日志消息-->
            <!--%n 换行符-->
            <charset>UTF-8</charset>
        </encoder>
    </appender>

    <!-- 时间滚动输出 level为 INFO 日志 -->
    <appender name="file" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 正在记录的日志文件的路径及文件名 -->
        <file>${logPath}/catalina.out</file>
        <!--日志文件输出格式-->
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n</pattern>
            <charset>UTF-8</charset>
        </encoder>
        <!-- 日志记录器的滚动策略，按日期，按大小记录 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <!-- 每天日志归档路径以及格式 -->
            <fileNamePattern>${logPath}/catalina.log-%d{yyyy-MM-dd}-%i.log</fileNamePattern>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>10MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
            <!--日志文件保留天数-->
            <maxHistory>7</maxHistory>
        </rollingPolicy>
        <!-- 此日志文件只记录info级别的 -->
        <!--<filter class="ch.qos.logback.classic.filter.LevelFilter">-->
        <!--    <level>info</level>-->
        <!--    <onMatch>ACCEPT</onMatch>-->
        <!--    <onMismatch>DENY</onMismatch>-->
        <!--</filter>-->
    </appender>

    <appender name="ip_record" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>info</level>
        </filter>
        <file>${logPath}/ip_record.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <FileNamePattern>${logPath}/ip_record.log.%d{yyyy-MM-dd}</FileNamePattern>
        </rollingPolicy>
        <encoder>
            <pattern>%yellow(%d{yyyy-MM-dd HH:mm:ss}) %red([%thread]) %highlight(%-5level) %cyan(%logger{50}) - %magenta(%msg) %n
            </pattern>
            <charset>UTF-8</charset>
        </encoder>
    </appender>

    <appender name="cache_log" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>info</level>
        </filter>
        <file>${logPath}/ip_record.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <FileNamePattern>${logPath}/cache_log.log.%d{yyyy-MM-dd}</FileNamePattern>
        </rollingPolicy>
        <encoder>
            <pattern>%yellow(%d{yyyy-MM-dd HH:mm:ss}) %red([%thread]) %highlight(%-5level) %cyan(%logger{50}) - %magenta(%msg) %n
            </pattern>
            <charset>UTF-8</charset>
        </encoder>
    </appender>

    <appender name="ex_log" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>error</level>
        </filter>
        <file>${logPath}/ex_log.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <FileNamePattern>${logPath}/ex_log.%d{yyyy-MM-dd}</FileNamePattern>
        </rollingPolicy>
        <encoder>
            <pattern>%yellow(%d{yyyy-MM-dd HH:mm:ss}) %red([%thread]) %highlight(%-5level) %cyan(%logger{50}) - %magenta(%msg) %n
            </pattern>
            <charset>UTF-8</charset>
        </encoder>
    </appender>


    <!-- 向kafka输出日志，dev环境下不启动kafka注意关闭-->
    <!--    <appender name="KafkaAppender" class="com.github.danielwegener.logback.kafka.KafkaAppender">-->
    <!--        <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">-->
    <!--            <providers class="net.logstash.logback.composite.loggingevent.LoggingEventJsonProviders">-->
    <!--                <pattern>-->
    <!--                    <pattern>-->
    <!--                        {"app":"${APP}",-->
    <!--                        "profile":"${PROFILES_ACTIVE}",-->
    <!--                        "thread": "%thread",-->
    <!--                        "logger": "%logger{5}",-->
    <!--                        "message":"%msg",-->
    <!--                        "app_name":"${APP_NAME}",-->
    <!--                        "env_name":"${ENV_NAME}",-->
    <!--                        "hostname":"${HOSTNAME}",-->
    <!--                        "captain_seq":"${CAPTAIN_SEQ}",-->
    <!--                        "captain_gen":"${CAPTAIN_GEN}",-->
    <!--                        "build_name":"${BUILD_NAME}",-->
    <!--                        "build_git_version":"${BUILD_GIT_VERSION}",-->
    <!--                        "build_git_hash":"${BUILD_GIT_HASH}",-->
    <!--                        "build_timestamp":"${BUILD_TIMESTAMP}",-->
    <!--                        "date":"%d{yyyy-MM-dd HH:mm:ss.SSS}",-->
    <!--                        "level":"%level",-->
    <!--                        "stack_trace":"%exception"-->
    <!--                        }-->
    <!--                    </pattern>-->
    <!--                </pattern>-->
    <!--            </providers>-->
    <!--        </encoder>-->
    <!--        <topic>roudblogLog</topic>-->
    <!--        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy" />-->
    <!--        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy" />-->
    <!--        <producerConfig>bootstrap.servers=localhost:9092</producerConfig>-->
    <!--        <producerConfig>retries=1</producerConfig>-->
    <!--        　　　 　  <producerConfig>batch-size=16384</producerConfig>-->
    <!--        　　　 　  <producerConfig>buffer-memory=33554432</producerConfig>-->
    <!--        　　　 　  <producerConfig>properties.max.request.size==2097152</producerConfig>-->
    <!--        <appender-ref ref="console"/>-->
    <!--    </appender>-->

    <!--    <logger level="info" addtivity="false" name="ip_record">-->
    <!--        <appender-ref ref="KafkaAppender"/>-->
    <!--    </logger>-->

    <logger level="info" addtivity="false" name="ip_record">
        <appender-ref ref="ip_record"/>
    </logger>

    <logger level="info" addtivity="false" name="ip_record">
        <appender-ref ref="cache_log"/>
    </logger>

    <logger level="error" addtivity="false" name="ex_log">
        <appender-ref ref="ex_log"/>
    </logger>

    <!-- 将文件输出设置成异步输出 -->
    <!--<appender name="async-file" class="ch.qos.logback.classic.AsyncAppender">-->
    <!--    &lt;!&ndash; 不丢失日志.默认的,如果队列的80%已满,则会丢弃TRACT、DEBUG、INFO级别的日志 &ndash;&gt;-->
    <!--    <discardingThreshold>0</discardingThreshold>-->
    <!--    &lt;!&ndash; 更改默认的队列的深度,该值会影响性能.默认值为256 &ndash;&gt;-->
    <!--    <queueSize>256</queueSize>-->
    <!--    &lt;!&ndash; 添加附加的appender,最多只能添加一个 &ndash;&gt;-->
    <!--    <appender-ref ref="file"/>-->
    <!--</appender>-->
    <!--
        <logger>用来设置某一个包或者具体的某一个类的日志打印级别、
        以及指定<appender>。<logger>仅有一个name属性，
        一个可选的level和一个可选的addtivity属性。
        name:用来指定受此logger约束的某一个包或者具体的某一个类。
        level:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF，
              还有一个特俗值INHERITED或者同义词NULL，代表强制执行上级的级别。
              如果未设置此属性，那么当前logger将会继承上级的级别。
        addtivity:是否向上级logger传递打印信息。默认是true。
    -->
    <!--<logger name="org.springframework.web" level="info"/>-->
    <!--<logger name="org.springframework.scheduling.annotation.ScheduledAnnotationBeanPostProcessor" level="INFO"/>-->
    <!--
        使用mybatis的时候，sql语句是debug下才会打印，而这里我们只配置了info，所以想要查看sql语句的话，有以下两种操作：
        第一种把<root level="info">改成<root level="DEBUG">这样就会打印sql，不过这样日志那边会出现很多其他消息
        第二种就是单独给dao下目录配置debug模式，代码如下，这样配置sql语句会打印，其他还是正常info级别：
     -->

    <!--
        root节点是必选节点，用来指定最基础的日志输出级别，只有一个level属性
        level:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF，
        不能设置为INHERITED或者同义词NULL。默认是DEBUG
        可以包含零个或多个元素，标识这个appender将会添加到这个logger。
    -->


    <root level="debug">
        <appender-ref ref="console" />
        <appender-ref ref="file" />
    </root>


</configuration>